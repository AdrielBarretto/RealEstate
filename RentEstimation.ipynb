{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19677c11-486d-4bb0-b2e9-6a8a383b4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import polars as pl\n",
    "import numpy as np \n",
    "import sklearn as sc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.stats import invwishart\n",
    "from scipy.stats import invgamma\n",
    "import math as math \n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "from keras.utils import Sequence\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# import tensorflow as tf\n",
    "# import keras\n",
    "from sklearn.metrics import r2_score\n",
    "import json\n",
    "import gc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9eab5-9d57-4914-86f8-b2ec535d7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned.dropna(inplace = True)\n",
    "samps = pd.read_csv('dewey_combined_clean.csv')\n",
    "zip_counts = samps.groupby(\"ZIP\")[\"ZIP\"].transform(\"count\")\n",
    "# cleaned = samps[zip_counts > 30]\n",
    "cleaned = samps[zip_counts > 50]\n",
    "del samps\n",
    "gc.collect()\n",
    "cols = [\n",
    "    \"BUILDING_TYPE_APT\", \"BUILDING_TYPE_COMM\", \"BUILDING_TYPE_CON\",\n",
    "    \"BUILDING_TYPE_SFR\", \"GARAGE_Y\", \"POOL_Y\"\n",
    "]\n",
    "cleaned[cols] = cleaned[cols].replace(\n",
    "    {\"True\": 1, \"False\": 0, \"true\": 1, \"false\": 0}\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8faf0-4c02-4942-a914-192a023c1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature generation\n",
    "num_features = ['BEDS','BATHS','SQFT','BUILDING_TYPE_APT',\n",
    "                'BUILDING_TYPE_COMM','BUILDING_TYPE_CON',\n",
    "                'BUILDING_TYPE_SFR','GARAGE_Y','POOL_Y','TIME']\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "num_scaler = StandardScaler()\n",
    "\n",
    "def get_batches(df, batch_size=500_000):\n",
    "    n = len(df)\n",
    "    for i in range(0, n, batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        X_zip = encoder.transform(batch[['ZIP']])  # stays sparse\n",
    "        X_num = batch[num_features].astype(np.float32)\n",
    "        X_num_scaled = num_scaler.transform(batch[num_features])\n",
    "        X_num_sparse = csr_matrix(X_num_scaled)  # convert to sparse for hstack\n",
    "\n",
    "        # Combine sparse + numeric\n",
    "        X = hstack([X_zip, X_num_sparse], format='csr')\n",
    "\n",
    "        # Target\n",
    "        y = batch['logrent'].values\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2468d51-1730-4e7b-92be-b4053f1f01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDRegressor(\n",
    "    loss=\"squared_error\",\n",
    "    penalty=None,\n",
    "    max_iter=1,      # 1 epoch per partial_fit call\n",
    "    learning_rate='invscaling',\n",
    "    eta0=0.01,\n",
    "    random_state=24\n",
    ")\n",
    "# for X_batch, y_batch in get_batches(cleaned):\n",
    "#     model.partial_fit(X_batch, y_batch)\n",
    "\n",
    "train_idx, test_idx = train_test_split(\n",
    "    cleaned.index, test_size=0.2, random_state=24\n",
    ")\n",
    "\n",
    "train_df = cleaned.loc[train_idx]\n",
    "test_df  = cleaned.loc[test_idx]\n",
    "encoder.fit(train_df[['ZIP']])\n",
    "num_scaler.fit(train_df[num_features])\n",
    "for X_batch, y_batch in get_batches(train_df):\n",
    "    model.partial_fit(X_batch, y_batch)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for X_batch, y_batch in get_batches(test_df):\n",
    "    y_true.append(y_batch)\n",
    "    y_pred.append(model.predict(X_batch))\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8eafa-d18e-4a66-a1cf-33c881f2557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RÂ²:\", r2)\n",
    "\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714b92a-0dac-411c-a553-b3812ef62fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = model\n",
    "\n",
    "zip_feature_names = encoder.get_feature_names_out(['ZIP'])\n",
    "num_feature_names = num_features\n",
    "\n",
    "all_features = list(zip_feature_names) + list(num_feature_names)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"coef\": sgd.coef_\n",
    "})\n",
    "\n",
    "print(\"Intercept:\", sgd.intercept_[0])\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bbe22-4197-4813-aa93-b3a36a570b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipcode level regressions\n",
    "listofzips = cleaned[\"ZIP\"].unique()\n",
    "mse_list  = []\n",
    "ct = 0\n",
    "for each in listofzips:\n",
    "    ct += 1\n",
    "    if ct%1000 == 0:\n",
    "        print(each)\n",
    "    gc.collect()\n",
    "    cleaned1 = cleaned[cleaned['ZIP'] == each]\n",
    "    X = cleaned1[['BEDS','BATHS','SQFT', 'BUILDING_TYPE_APT','BUILDING_TYPE_COMM', 'BUILDING_TYPE_CON','BUILDING_TYPE_SFR', 'GARAGE_Y', 'POOL_Y','TIME']].fillna(0)\n",
    "    X.dropna(inplace = True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, cleaned1['logrent'], test_size=0.2)\n",
    "    y_test = np.array(y_test)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = 0\n",
    "    for i in range(len(y_test)):\n",
    "        mse+= (y_test[i]-y_pred[i])**2\n",
    "    mse = mse/len(y_test)\n",
    "    mse_list.append(mse)\n",
    "    \n",
    "    # #Neural Network\n",
    "    # x = sc.preprocessing.StandardScaler().fit_transform(X)\n",
    "    # y = (np.array(cleaned1['logrent']) - np.mean(np.array(cleaned1['logrent'])))/np.std(np.array(cleaned1['logrent']))\n",
    "    # nX_train, nX_test, ny_train, ny_test = sc.model_selection.train_test_split(x, y, test_size=0.2)\n",
    "    # hidden_units = 20\n",
    "    # activation = 'sigmoid'\n",
    "    # learning_rate = 0.05\n",
    "    # epochs = 25\n",
    "    # # [5,10,25,50,75,100]\n",
    "    # batch_size = 32\n",
    "    # neural_model = keras.models.Sequential()\n",
    "    # neural_model.add(keras.layers.Dense(input_dim=len(X.columns),\n",
    "    #                              units=hidden_units,\n",
    "    #                              activation=activation))\n",
    "    # # add the output layer\n",
    "    # neural_model.add(keras.layers.Dense(input_dim=hidden_units,\n",
    "    #                              units=1))\n",
    "    # # define our loss function and optimizer\n",
    "    # neural_model.compile(loss='MeanSquaredError',\n",
    "    #               # Adam is a kind of gradient descent\n",
    "    #               optimizer=keras.optimizers.Adam(learning_rate=.01),\n",
    "    #               metrics=['mse'])\n",
    "    # execute = neural_model.fit(nX_train, ny_train, epochs=epochs, batch_size=batch_size)\n",
    "    # test_acc = neural_model.evaluate(nX_test, ny_test)\n",
    "    # neural_mse_list.append(test_acc)\n",
    "average = np.mean(mse_list)\n",
    "# average2 = np.mean(neural_mse_list)\n",
    "print(average)\n",
    "# print(average2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48739226-74ca-4438-bff9-eba1f4ea0979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0de095-df2c-4e11-8958-43fc9a458c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02520242-1c7f-49e7-90e5-a61c06a586ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9d4eb-d578-4edd-92a1-e17320b8625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct version\n",
    "# cleaned = cleaned[zip_counts > 50]\n",
    "#Bayesian loop samples\n",
    "#MSE Calc does mse calcs\n",
    "#Rhat calculates rubin gelman statistic\n",
    "#Maxrhat gets the highest rhat\n",
    "#Listinitializer sets up the list of rhats \n",
    "#Listappender adds it \n",
    "def bayesianloop(sigma, sigmagroup, xtx, xty, beta, invlamb, betaindividual, bigsigma, xlist, ylist, xi,sigs,m,mu, p, size):\n",
    "    siginv = np.linalg.inv(sigma)\n",
    "    for j in range(m):\n",
    "        cov = np.linalg.inv(siginv+(1/sigmagroup[j])*xtx[j].values)\n",
    "        mean = cov@(siginv@beta+(1/sigmagroup[j])*xty[j].values)\n",
    "        betaindividual[j] = np.random.multivariate_normal(mean, cov)\n",
    "    covbeta = np.linalg.inv(invlamb+m*siginv)\n",
    "    sum_betas = np.sum(betaindividual, axis=0)\n",
    "    mubeta = covbeta@(invlamb@mu+siginv@sum_betas)\n",
    "    beta = np.random.multivariate_normal(mubeta, covbeta)\n",
    "    summy = bigsigma.copy()\n",
    "    for j in range(m):\n",
    "        a = betaindividual[j]-beta\n",
    "        summy+=np.outer(a, a)\n",
    "    sigma = invwishart.rvs(df=p+2+m, scale=summy)\n",
    "    for j in range(m):\n",
    "        second = ylist[j]-xlist[j].values@betaindividual[j]\n",
    "        t = [x ** 2 for x in second]\n",
    "        secondary = np.sum(np.array(t))\n",
    "        sigmagroup[j] = invgamma.rvs(a =.5*(1+size[j]), scale = .5*(xi+secondary) , size =1)[0]\n",
    "    inversesigma =  (1/sigs)+.5*np.sum(1/np.array(sigmagroup))\n",
    "    xi = np.random.gamma(1+.5*m,1/inversesigma)\n",
    "    return sigma, beta, betaindividual, xi, sigmagroup\n",
    "def msecalc(betaindividual1,xlisttest,ylisttest):\n",
    "    ypred = []\n",
    "    for j in range(m):\n",
    "        ypred.append(xlisttest[j]@betaindividual1[j])\n",
    "        ypred[j] = np.array(ypred[j])\n",
    "        ylisttest[j] = np.array(ylisttest[j])\n",
    "    count = 0\n",
    "    msetemp = 0 \n",
    "    for j in range(m):\n",
    "        for i in range(len(ypred[j])):\n",
    "            msetemp+= (ypred[j][i]-ylisttest[j][i])**2\n",
    "            count+=1\n",
    "    mse = (1/count)*msetemp\n",
    "    return mse\n",
    "def rhat(lister, num):\n",
    "    n = len(lister[0])\n",
    "    mean = []\n",
    "    sdchain = []\n",
    "    for i in range(num):\n",
    "        mean.append(np.mean(np.array(lister[i])))\n",
    "        sdchain.append(np.var(np.array(lister[i]), ddof=1))\n",
    "    B = n*np.var(np.array(mean),ddof=1)\n",
    "    W = np.mean(sdchain)\n",
    "    varpsiy = ((n-1)/n)*W+(1/n)*B\n",
    "    rhat = np.sqrt(varpsiy/W)\n",
    "    return rhat\n",
    "def maxrhat(lister,num,param):\n",
    "    maxval = 0\n",
    "    for i in range(len(lister)):\n",
    "            a = rhat(lister[i],num)\n",
    "            if a>maxval:\n",
    "                maxval = a\n",
    "    return maxval\n",
    "def listinitializer(betaindividual1, betaindividual2, betaindividual3,betaindividual4):\n",
    "    lister = []\n",
    "    for i in range(len(betaindividual1)):\n",
    "        for j in range(len(betaindividual1[0])):\n",
    "            lister.append([[betaindividual1[i][j]],[betaindividual2[i][j]], [betaindividual3[i][j]], [betaindividual4[i][j]] ])\n",
    "    return lister\n",
    "def listappender(lister,param,betaindividual1, betaindividual2, betaindividual3,betaindividual4,m):\n",
    "    for i in range(m):\n",
    "        for j in range(param):\n",
    "            z = i*param+j\n",
    "            lister[z][0].append(betaindividual1[i][j])\n",
    "            lister[z][1].append(betaindividual2[i][j])\n",
    "            lister[z][2].append(betaindividual3[i][j])\n",
    "            lister[z][3].append(betaindividual4[i][j])\n",
    "    return lister\n",
    "'''\n",
    "Testing\n",
    "a = [[1,2],[1,2], [1,2], [1,2]]\n",
    "b = [[3,4],[3,4], [3,4], [3,4]]\n",
    "c = [[5,6],[5,6], [5,6], [5,6]]\n",
    "d = [[7,8], [7,8], [7,8], [7,8]]\n",
    "q = listinitializer(a,b,c,d)\n",
    "print(q)\n",
    "q = listappender(q,2,a,b,c,d,4)\n",
    "print(q)\n",
    "print(len(q))\n",
    "'''\n",
    "# sample = pd.read_csv(\"a.csv\")\n",
    "# samps = sample.drop(['GRANITE', 'STAINLESS','GYM','DOORMAN','FURNISHED','LAUNDRY', 'CLUBHOUSE','LATITUDE','LONGITUDE','DESCRIPTION', 'GARAGE_COUNT','ADDRESS', 'COMPANY','ID','NEIGHBORHOOD','SCRAPED_TIMESTAMP','YEAR_BUILT','AVAILABLE_AT','AVAILABILITY_STATUS','ID'], axis=1)\n",
    "# samps['YEAR'] =pd.to_datetime(samps['DATE_POSTED'])\n",
    "# samps['MONTH'] = pd.to_numeric(samps['YEAR'].dt.month)\n",
    "# samps['YEAR'] = pd.to_numeric(samps['YEAR'].dt.year)-2014\n",
    "\n",
    "#samps['MONTH'] = pd.to_numeric(pd.to_datetime(samps['DATE_POSTED'],dayfirst=True, format = \"%m\"))\n",
    "# samps['TIME'] = 12*samps['YEAR']-1+samps['MONTH']\n",
    "# one = pd.get_dummies(samps,columns = ['BUILDING_TYPE'], drop_first=False)\n",
    "# new_samps = pd.get_dummies(one,columns = ['GARAGE','POOL'], drop_first=True)\n",
    "# zen = len(samps['ZIP'].unique())\n",
    "# new_samps.drop(columns = [ 'BUILDING_TYPE_TIME', 'BUILDING_TYPE_MH', 'BUILDING_TYPE_TH','DATE_POSTED','YEAR','MONTH'],axis =1,inplace=True)\n",
    "# new_samps[\"logrent\"] = np.log(new_samps[\"RENT_PRICE\"])\n",
    "# cleaned = new_samps.groupby('ZIP').filter(lambda x: len(x) > 30)\n",
    "# cleaned.dropna(inplace = True)\n",
    "# cleaned = cleaned.groupby('ZIP').filter(lambda x: len(x) > 30)\n",
    "#Same cleaning \n",
    "# t = ['BEDS','BATHS','SQFT', 'BUILDING_TYPE_APT','BUILDING_TYPE_COMM', 'BUILDING_TYPE_CON','BUILDING_TYPE_SFR', 'GARAGE_Y', 'POOL_Y', 'ZIP','logrent']\n",
    "# cleaned = cleaned[t].replace({True:1,False:0})\n",
    "cleaned['INTERCEPT'] = 1\n",
    "t = ['BEDS','BATHS','SQFT', 'BUILDING_TYPE_APT','BUILDING_TYPE_COMM', 'BUILDING_TYPE_CON','BUILDING_TYPE_SFR', 'GARAGE_Y', 'POOL_Y', 'ZIP', 'INTERCEPT']\n",
    "#b = np.array(cleaned['logrent'])\n",
    "print(\"Cleaning done\")\n",
    "#split data\n",
    "na = cleaned[t].shape[0]\n",
    "p =  cleaned[t].shape[1]\n",
    "'''\n",
    "Xtrain, X_test, y, ytest= train_test_split(cleaned[t], b, test_size=0.2, random_state = 24)\n",
    "X = Xtrain[['BEDS','BATHS','SQFT', 'BUILDING_TYPE_APT','BUILDING_TYPE_COMM', 'BUILDING_TYPE_CON','BUILDING_TYPE_SFR', 'GARAGE_Y', 'POOL_Y', 'INTERCEPT']]\n",
    "model = LinearRegression(fit_intercept =False)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "mu = model.coef_.flatten()\n",
    "print(mu)\n",
    "na = X.shape[0]\n",
    "p = X.shape[1]\n",
    "f = np.sum((y-y_pred)**2)\n",
    "sigs = f/(na-p-1)\n",
    "print(sigs)\n",
    "X1=X.values\n",
    "lamb = (na*p*sigs)*np.linalg.inv(X1.T@X1)\n",
    "'''\n",
    "mu = np.array([\n",
    "    -1.00774303e-01,  1.67737087e-01,  4.58936502e-04,  9.81264571e-03, \n",
    "    -2.93593061e-02,  1.29088669e-01, -8.38429325e-02,  1.52842902e-01, \n",
    "    -8.40472908e-02,  6.51888483e+00\n",
    "])\n",
    "\n",
    "sigs = 0.1833664 \n",
    "\n",
    "lamb = np.array([\n",
    "    [ 6.20035246e+00, -3.58903197e+00, -4.51962845e-03,  5.72635667e-01,  2.69108986e+00,  5.88809435e-01, -2.42599313e+00,  2.47122921e-01,  1.15872015e-01, -1.38180939e+00],\n",
    "    [-3.58903197e+00,  1.25802700e+01, -7.32749460e-03,  1.15797905e+00,  2.73554620e+00, -1.50989972e-01,  3.78100893e+00, -1.11963954e-01, -3.80338954e-01, -6.25751273e+00],\n",
    "    [-4.51962845e-03, -7.32749460e-03,  2.25171969e-05,  1.28166905e-03, -6.73213789e-03,  9.82066580e-04, -4.41129577e-03, -4.79984530e-04,  7.31604797e-05, -4.90178389e-03],\n",
    "    [ 5.72635667e-01,  1.15797905e+00,  1.28166905e-03,  4.16213837e+01,  3.84308200e+01,  3.84443713e+01,  3.65729396e+01, -6.83973916e-01, -8.72583272e-01, -4.27492601e+01],\n",
    "    [ 2.69108986e+00,  2.73554620e+00, -6.73213789e-03,  3.84308200e+01,  1.74212185e+03,  3.80719223e+01,  3.81990500e+01, -7.12978347e-02,  7.00553569e-01, -4.06496000e+01],\n",
    "    [ 5.88809435e-01, -1.50989972e-01,  9.82066580e-04,  3.84443713e+01,  3.80719223e+01,  1.00679003e+02,  3.70421028e+01,  1.29626583e+00, -2.42306366e-01, -4.01466318e+01],\n",
    "    [-2.42599313e+00,  3.78100893e+00, -4.41129577e-03,  3.65729396e+01,  3.81990500e+01,  3.70421028e+01,  5.25767088e+01,  6.30170310e-01,  1.46100043e+00, -3.43151619e+01],\n",
    "    [ 2.47122921e-01, -1.11963954e-01, -4.79984530e-04, -6.83973916e-01, -7.12978347e-02,  1.29626583e+00,  6.30170310e-01,  4.83268903e+01, -4.85717321e+00,  1.19654995e-01],\n",
    "    [ 1.15872015e-01, -3.80338954e-01,  7.31604797e-05, -8.72583272e-01,  7.00553569e-01, -2.42306366e-01,  1.46100043e+00, -4.85717321e+00,  9.36057673e+00, -2.05063007e+00],\n",
    "    [-1.38180939e+00, -6.25751273e+00, -4.90178389e-03, -4.27492601e+01, -4.06496000e+01, -4.01466318e+01, -3.43151619e+01,  1.19654995e-01, -2.05063007e+00,  5.97457038e+01]\n",
    "])\n",
    "invlamb = np.linalg.inv(lamb)\n",
    "sigma = lamb\n",
    "bigsigma = lamb\n",
    "beta = mu\n",
    "xi = math.sqrt(sigs)\n",
    "t = ['BEDS','BATHS','SQFT', 'BUILDING_TYPE_APT','BUILDING_TYPE_COMM', 'BUILDING_TYPE_CON','BUILDING_TYPE_SFR', 'GARAGE_Y', 'POOL_Y', 'INTERCEPT']\n",
    "print(\"Linear done\")\n",
    "listofzips = cleaned['ZIP'].unique()\n",
    "m = len(listofzips)\n",
    "betaindividual = []\n",
    "ylist= []\n",
    "xlist = []\n",
    "xlisttest = []\n",
    "ylisttest = [] \n",
    "xtx = []\n",
    "xty = []\n",
    "size =[]\n",
    "for each in listofzips:\n",
    "    xyz = cleaned[cleaned['ZIP'] == each]\n",
    "    X = xyz[t]\n",
    "    y = np.array(xyz['logrent'])\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state = 24)\n",
    "    n = X.shape[0]\n",
    "    size.append(n)\n",
    "    xlist.append(Xtrain)\n",
    "    ylist.append(ytrain)\n",
    "    xlisttest.append(Xtest)\n",
    "    ylisttest.append(ytest)\n",
    "    xtx.append(Xtrain.T@Xtrain)\n",
    "    xty.append(Xtrain.T@ytrain)\n",
    "\n",
    "print(\"Bayesian time!\")\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "xi1 = np.random.gamma(1, (1/sigs))\n",
    "xi2 = np.random.gamma(1, (1/sigs))\n",
    "xi3 = np.random.gamma(1, (1/sigs))\n",
    "xi4 = np.random.gamma(1, (1/sigs))\n",
    "sigmagroup1 = invgamma.rvs(a =.5, scale = .5*xi1, size =m)\n",
    "sigmagroup2 = invgamma.rvs(a =.5, scale = .5*xi2, size =m)\n",
    "sigmagroup3 = invgamma.rvs(a =.5, scale = .5*xi3, size =m)\n",
    "sigmagroup4 = invgamma.rvs(a =.5, scale = .5*xi4, size =m)\n",
    "\n",
    "sigma1 = invwishart.rvs(df=10, scale=bigsigma)\n",
    "sigma2 = invwishart.rvs(df=10, scale=bigsigma)\n",
    "sigma3 = invwishart.rvs(df=10, scale=bigsigma)\n",
    "sigma4 = invwishart.rvs(df=10, scale=bigsigma)\n",
    "beta1 = np.random.multivariate_normal(mean = mu, cov = lamb)\n",
    "beta2 = np.random.multivariate_normal(mean = mu, cov = lamb)\n",
    "beta3 = np.random.multivariate_normal(mean = mu, cov = lamb)\n",
    "beta4 = np.random.multivariate_normal(mean = mu, cov = lamb)\n",
    "betaindividual1 = []\n",
    "betaindividual2 = []\n",
    "betaindividual3 = []\n",
    "betaindividual4 = []\n",
    "for j in range(m):\n",
    "    betaindividual1.append(np.random.multivariate_normal(mean = beta1, cov = sigma1))\n",
    "    betaindividual2.append(np.random.multivariate_normal(mean = beta2, cov = sigma2))\n",
    "    betaindividual3.append(np.random.multivariate_normal(mean = beta3, cov = sigma3))\n",
    "    betaindividual4.append(np.random.multivariate_normal(mean = beta4, cov = sigma4))\n",
    "print(\"Initialize done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c683f-7693-4011-a199-ca976333457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial sampling/Burn in \n",
    "for i in range(500):\n",
    "    print(i)\n",
    "    sigma1, beta1, betaindividual1, xi1, sigmagroup1 = bayesianloop(sigma1, sigmagroup1, xtx, xty, beta1, invlamb, betaindividual1, bigsigma, xlist, ylist, xi1,sigs, m,mu,p,size)\n",
    "    sigma2, beta2, betaindividual2, xi2, sigmagroup2 = bayesianloop(sigma2, sigmagroup2, xtx, xty, beta2, invlamb, betaindividual2, bigsigma, xlist, ylist, xi2,sigs,m,mu,p,size)\n",
    "    sigma3, beta3, betaindividual3, xi3, sigmagroup3 = bayesianloop(sigma3, sigmagroup3, xtx, xty, beta3, invlamb, betaindividual3, bigsigma, xlist, ylist, xi3,sigs,m,mu,p,size)\n",
    "    sigma4, beta4, betaindividual4, xi4, sigmagroup4 = bayesianloop(sigma4, sigmagroup4, xtx, xty, beta4, invlamb, betaindividual4, bigsigma, xlist, ylist, xi4,sigs,m,mu,p,size)\n",
    "print(\"Sampling done\")\n",
    "maxrhats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718de1e6-dcc2-42ea-a20a-2cc52d7fb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "totallist = listinitializer(betaindividual1,betaindividual2, betaindividual3, betaindividual4)\n",
    "r = 100000\n",
    "i=1\n",
    "while r>1.05:\n",
    "    sigma1, beta1, betaindividual1, xi1, sigmagroup1 = bayesianloop(sigma1, sigmagroup1, xtx, xty, beta1, invlamb, betaindividual1, bigsigma, xlist, ylist, xi1,sigs,m,mu,p,size)\n",
    "    sigma2, beta2, betaindividual2, xi2, sigmagroup2 = bayesianloop(sigma2, sigmagroup2, xtx, xty, beta2, invlamb, betaindividual2, bigsigma, xlist, ylist, xi2,sigs,m,mu,p,size)\n",
    "    sigma3, beta3, betaindividual3, xi3, sigmagroup3 = bayesianloop(sigma3, sigmagroup3, xtx, xty, beta3, invlamb, betaindividual3, bigsigma, xlist, ylist, xi3,sigs,m,mu,p,size)\n",
    "    sigma4, beta4, betaindividual4, xi4, sigmagroup4 = bayesianloop(sigma4, sigmagroup4, xtx, xty, beta4, invlamb, betaindividual4, bigsigma, xlist, ylist, xi4,sigs,m,mu,p,size)\n",
    "    totallist = listappender(totallist,10,betaindividual1, betaindividual2, betaindividual3,betaindividual4,m)\n",
    "    if i%10 == 0:\n",
    "        r = maxrhat(totallist,num = 4, param = 10)\n",
    "        maxrhats.append(r)\n",
    "        print(r)\n",
    "    i+=1\n",
    "print(\"convergence done\")\n",
    "print(\"MSE:\"+str(msecalc(betaindividual4,xlisttest, ylisttest)))\n",
    "print(\"MSE:\"+str(msecalc(betaindividual3,xlisttest, ylisttest)))\n",
    "print(\"MSE:\"+str(msecalc(betaindividual2,xlisttest, ylisttest)))\n",
    "print(\"MSE:\"+str(msecalc(betaindividual1,xlisttest, ylisttest)))\n",
    "finallist = []\n",
    "for i in range(30):\n",
    "    sigma1, beta1, betaindividual1, xi1, sigmagroup1 = bayesianloop(sigma1, sigmagroup1, xtx, xty, beta1, invlamb, betaindividual1, bigsigma, xlist, ylist, xi1,sigs,m,mu,p,size)\n",
    "    sigma2, beta2, betaindividual2, xi2, sigmagroup2 = bayesianloop(sigma2, sigmagroup2, xtx, xty, beta2, invlamb, betaindividual2, bigsigma, xlist, ylist, xi2,sigs,m,mu,p,size)\n",
    "    sigma3, beta3, betaindividual3, xi3, sigmagroup3 = bayesianloop(sigma3, sigmagroup3, xtx, xty, beta3, invlamb, betaindividual3, bigsigma, xlist, ylist, xi3,sigs,m,mu,p,size)\n",
    "    sigma4, beta4, betaindividual4, xi4, sigmagroup4 = bayesianloop(sigma4, sigmagroup4, xtx, xty, beta4, invlamb, betaindividual4, bigsigma, xlist, ylist, xi4,sigs,m,mu,p,size)\n",
    "    finallist.append(betaindividual1)\n",
    "    finallist.append(betaindividual2)\n",
    "    finallist.append(betaindividual3)\n",
    "    finallist.append(betaindividual4)\n",
    "print(\"final mean\")\n",
    "finalbetalist = np.mean(finallist, axis=0)\n",
    "print(\"Final MSE:\"+str(msecalc(finalbetalist,xlisttest, ylisttest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbca9a6-12cf-4cd9-b46a-0070041e9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([finallist[i][7840][5]for i in range(120)])\n",
    "plt.ylim([-0.4,0.2])\n",
    "plt.ylabel('Beta Value')\n",
    "plt.title('Convergence Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49981ce-4014-445c-8ee3-ffd93480c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([finallist[i][1660][3]for i in range(120)])\n",
    "plt.ylim([-0.4,0.2])\n",
    "plt.ylabel('Beta Value')\n",
    "plt.title('Convergence Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
